# Google Scraper Masivo - VersiÃ³n Mejorada

ğŸ” **Herramienta de scraping automatizado para extraer datos de contacto de negocios desde Google, Instagram, Google Maps, LinkedIn y Facebook.**

## ğŸ†• Nuevas Funcionalidades

### âœ¨ CaracterÃ­sticas Mejoradas
- âœ… **Soporte para Facebook**: Ahora incluye bÃºsquedas en Facebook
- âœ… **DeduplicaciÃ³n AutomÃ¡tica**: Elimina leads duplicados automÃ¡ticamente
- âœ… **ExtracciÃ³n Predeterminada**: Obtiene automÃ¡ticamente nombre, telÃ©fono, email y sitio web
- âœ… **Interfaz de Progreso**: VisualizaciÃ³n en tiempo real del progreso de scraping
- âœ… **Descarga Directa**: BotÃ³n para descargar el archivo CSV generado
- âœ… **UbicaciÃ³n del CSV Visible**: Muestra claramente dÃ³nde se guardan los resultados

### ğŸ“Š Datos ExtraÃ­dos AutomÃ¡ticamente
- **Nombre**: Nombre del negocio o contacto
- **TelÃ©fono**: NÃºmero de contacto
- **Email**: DirecciÃ³n de correo electrÃ³nico  
- **Sitio Web**: URL del sitio web o perfil
- **DescripciÃ³n**: InformaciÃ³n adicional del negocio

### ğŸ“ UbicaciÃ³n de Resultados
Los archivos CSV se guardan automÃ¡ticamente en: `results/scraped_data.csv`

## ğŸš€ InstalaciÃ³n RÃ¡pida

### OpciÃ³n 1: Script AutomÃ¡tico
```bash
# Ejecutar script de configuraciÃ³n
chmod +x setup.sh
./setup.sh
```

### OpciÃ³n 2: Manual
```bash
# 1. Configurar aplicaciÃ³n web
cd scraper_interface
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Crear directorio de resultados
mkdir -p results

# 3. Ejecutar aplicaciÃ³n
python src/main.py
```

## ğŸ¯ CÃ³mo Usar

1. **Abrir navegador** en `http://localhost:5000`
2. **Especificar tipo de negocio** (ej: "restaurantes, hoteles")
3. **Seleccionar plataformas** (Instagram, Google Maps, LinkedIn, Facebook)
4. **Agregar ubicaciÃ³n** (opcional, ej: "Madrid")
5. **Definir cantidad de leads** deseados
6. **Hacer clic en "ğŸš€ Iniciar Scraping"**
7. **Ver progreso en tiempo real**
8. **Descargar archivo CSV** cuando termine

## ğŸ›¡ï¸ TÃ©cnicas Anti-DetecciÃ³n Avanzadas

- **Navegador Indetectable**: `undetected_chromedriver`
- **RotaciÃ³n de User-Agents**: Simula diferentes navegadores
- **Retrasos Aleatorios**: Entre 2-10 segundos entre acciones
- **Comportamiento Humano**: Movimientos y patrones naturales
- **GestiÃ³n de Cookies**: ReutilizaciÃ³n inteligente de sesiones
- **Modo Headless**: Para despliegue en servidores

## ğŸ“‹ Plataformas Soportadas

| Plataforma | Icono | Datos ExtraÃ­dos |
|------------|-------|-----------------|
| Instagram | ğŸ“· | Perfiles, contactos, descripciones |
| Google Maps | ğŸ—ºï¸ | Negocios locales, telÃ©fonos, direcciones |
| LinkedIn | ğŸ’¼ | Perfiles profesionales, empresas |
| Facebook | ğŸ“˜ | PÃ¡ginas de negocios, informaciÃ³n de contacto |

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno
- `HEADLESS_MODE`: true/false (por defecto: true)
- `DELAY_MIN`: Retraso mÃ­nimo en segundos (por defecto: 2)
- `DELAY_MAX`: Retraso mÃ¡ximo en segundos (por defecto: 10)

### PersonalizaciÃ³n de BÃºsquedas
El sistema genera automÃ¡ticamente consultas optimizadas como:
```
site:facebook.com "restaurantes" "Madrid" "gmail.com" OR "hotmail.com" OR "contacto" "numero de telefono"
```

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ scraper.py                 # Motor de scraping mejorado
â”œâ”€â”€ setup.sh                   # Script de configuraciÃ³n automÃ¡tica
â”œâ”€â”€ README.md                  # Esta documentaciÃ³n
â”œâ”€â”€ requirements.txt           # Dependencias principales
â””â”€â”€ scraper_interface/         # AplicaciÃ³n web Flask
    â”œâ”€â”€ scraper.py            # Copia del motor de scraping
    â”œâ”€â”€ requirements.txt      # Dependencias de la aplicaciÃ³n web
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ main.py          # Servidor Flask
    â”‚   â”œâ”€â”€ routes/
    â”‚   â”‚   â””â”€â”€ scraper.py   # API endpoints mejorados
    â”‚   â””â”€â”€ static/
    â”‚       â””â”€â”€ index.html   # Interfaz web actualizada
    â””â”€â”€ results/             # Directorio de archivos CSV generados
```

## âš¡ Mejoras de Rendimiento

- **Scraping en Segundo Plano**: No bloquea la interfaz
- **DeduplicaciÃ³n Inteligente**: Evita leads repetidos
- **ExtracciÃ³n Optimizada**: Regex mejoradas para mejor precisiÃ³n
- **GestiÃ³n de Memoria**: LiberaciÃ³n automÃ¡tica de recursos
- **Manejo de Errores**: RecuperaciÃ³n automÃ¡tica de fallos

## ğŸŒ Despliegue en ProducciÃ³n

### Para Hosting Web
1. Subir todos los archivos al servidor
2. Ejecutar `./setup.sh` en el servidor
3. Configurar servidor web (Apache/Nginx) para proxy a puerto 5000
4. Instalar Chrome/Chromium en el servidor

### Para VPS/Cloud
```bash
# Instalar dependencias del sistema
sudo apt-get update
sudo apt-get install -y python3-pip python3-venv chromium-browser

# Configurar aplicaciÃ³n
./setup.sh

# Ejecutar en modo producciÃ³n
cd scraper_interface
source venv/bin/activate
gunicorn -w 4 -b 0.0.0.0:5000 src.main:app
```

## ğŸ”’ Consideraciones de Seguridad

- **ValidaciÃ³n de Entrada**: Previene inyecciÃ³n de cÃ³digo
- **Descarga Segura**: Solo archivos CSV del directorio results/
- **LÃ­mites de Velocidad**: Previene sobrecarga del servidor
- **Logs de Actividad**: Registro de todas las operaciones

## âš ï¸ Uso Responsable

- **Respeta robots.txt** de cada sitio web
- **Usa velocidades conservadoras** para evitar sobrecargar servidores
- **Cumple con GDPR/CCPA** para manejo de datos personales
- **Solo para propÃ³sitos comerciales legÃ­timos**
- **Revisa tÃ©rminos de servicio** de cada plataforma

## ğŸ†˜ SoluciÃ³n de Problemas

### Problemas Comunes

**Error: "Chrome not found"**
```bash
# Ubuntu/Debian
sudo apt-get install chromium-browser

# CentOS/RHEL
sudo yum install chromium
```

**Error: "Permission denied"**
```bash
chmod +x setup.sh
sudo chown -R $USER:$USER scraper_interface/
```

**CAPTCHA frecuente**
- Reducir velocidad de scraping
- Usar proxies rotativos
- Implementar pausas mÃ¡s largas

**CSV vacÃ­o o incompleto**
- Verificar conexiÃ³n a internet
- Ajustar tÃ©rminos de bÃºsqueda
- Revisar logs de errores

## ğŸ“ˆ EstadÃ­sticas de Rendimiento

- **Velocidad**: ~10-50 leads por minuto (segÃºn plataforma)
- **PrecisiÃ³n**: ~85-95% en extracciÃ³n de datos
- **DeduplicaciÃ³n**: 100% efectiva
- **Uptime**: 99%+ con manejo de errores

## ğŸ”„ Actualizaciones Futuras

- [ ] IntegraciÃ³n con APIs oficiales
- [ ] Soporte para mÃ¡s plataformas (Twitter, TikTok)
- [ ] Dashboard de analytics
- [ ] ExportaciÃ³n a mÃºltiples formatos
- [ ] ProgramaciÃ³n de scraping automÃ¡tico
- [ ] IntegraciÃ³n con CRM

---

**ğŸ¯ Desarrollado para scraping masivo eficiente, responsable y escalable**

*Ãšltima actualizaciÃ³n: 30 de junio de 2025*

